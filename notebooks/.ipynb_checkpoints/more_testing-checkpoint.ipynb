{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from embedding_functions_hugo.embedding_functions import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowering done! \n",
      "Contractions removed!\n",
      "urls away!\n",
      "htmls too!\n",
      "asciis outta here!\n",
      "special  characs done\n",
      "puncts done\n",
      "long one...\n"
     ]
    }
   ],
   "source": [
    "df_gaming = pd.read_csv('../data/gaming.csv')\n",
    "df_satis = pd.read_csv('../data/SatisfactoryGame.csv')\n",
    "df_marauders = pd.read_csv('../data/MaraudersGame.csv')\n",
    "df_tarkov = pd.read_csv('../data/EscapefromTarkov.csv')\n",
    "df_politics = pd.read_csv('../data/politics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = df_politics.values[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowering done! \n",
      "Contractions removed!\n",
      "urls away!\n",
      "htmls too!\n",
      "asciis outta here!\n",
      "special  characs done\n",
      "puncts done\n",
      "long one...\n",
      "lowering done! \n",
      "Contractions removed!\n",
      "urls away!\n",
      "htmls too!\n",
      "asciis outta here!\n",
      "special  characs done\n",
      "puncts done\n",
      "long one...\n",
      "lowering done! \n",
      "Contractions removed!\n",
      "urls away!\n",
      "htmls too!\n",
      "asciis outta here!\n",
      "special  characs done\n",
      "puncts done\n",
      "long one...\n"
     ]
    }
   ],
   "source": [
    "for n in [df_politics, df_gaming, df_marauders, df_tarkov]:\n",
    "    n['cleaned_text'] = prep_pipeline(n, 'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Voikios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Voikios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Voikios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Creates the embeddings and stores them in the embeddings folder, can load with np.load()\n",
    "# for n, i in zip([df_gaming, df_marauders, df_tarkov, df_politics], ['gaming', 'marauders', 'tarkov', 'politics']):\n",
    "#     pol_embs= embed_comments(n['cleaned_text'])\n",
    "#     np.save(f'Embeddings/{i}_embeddings.npy', pol_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Voikios\\anaconda3\\envs\\Bachelor_Reddit\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)821d1/.gitattributes: 100%|██████████| 391/391 [00:00<00:00, 391kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 95.1kB/s]\n",
      "Downloading (…)8d01e821d1/README.md: 100%|██████████| 3.95k/3.95k [00:00<00:00, 1.98MB/s]\n",
      "Downloading (…)d1/added_tokens.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 2.00kB/s]\n",
      "Downloading (…)01e821d1/config.json: 100%|██████████| 625/625 [00:00<00:00, 625kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 122kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 438M/438M [00:37<00:00, 11.6MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 26.5kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 56.0kB/s]\n",
      "Downloading (…)821d1/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 558kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 399/399 [00:00<00:00, 200kB/s]\n",
      "Downloading (…)8d01e821d1/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 445kB/s]\n",
      "Downloading (…)1e821d1/modules.json: 100%|██████████| 229/229 [00:00<00:00, 229kB/s]\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Voikios\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# embed_comments(df1['comment_text'])\n",
    "#two_dims(df1['comment_text'])\n",
    "#tarkov = embed_comments(df_tarkov['comment_text'])\n",
    "#marauders = embed_comments(df_marauders['comment_text'])\n",
    "#satis = embed_comments(df_satis['comment_text'])\n",
    "#pol_embs= embed_comments(df_marauders['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(pol_embs), len(df_politics))\n",
    "usr_id = df_politics['comment_author']\n",
    "usr_dict = {}\n",
    "for i, j in zip(authors, pol_embs):\n",
    "    if i not in usr_dict.keys():\n",
    "        usr_dict[i] = []\n",
    "        usr_dict[i].append(j)\n",
    "    else:\n",
    "        usr_dict[i].append(j)\n",
    "len(usr_dict)\n",
    "len(pol_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in usr_dict:\n",
    "    g = len(usr_dict[i])\n",
    "    usr_dict[i] = sum(usr_dict[i])/g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pol_embs = list(usr_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dims(new_pol_embs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_comments(comments, cluster_count, pre_emb = False, scaler = False): \n",
    "    '''Takes a collection of comments, maps it to a 2d/1d space and clusters them in a user-determined amount of groups.'''\n",
    "    # imports \n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    import matplotlib.pyplot as plt\n",
    "    from numpy import unique\n",
    "    from numpy import where\n",
    "    #load_data\n",
    "    if not pre_emb:\n",
    "        sentences = embed_comments(comments)\n",
    "    else:\n",
    "        sentences = comments\n",
    "    # Initialize model\n",
    "    model = KMeans(n_clusters=cluster_count)\n",
    "\n",
    "    if scaler:\n",
    "        sentences = scaler.fit_transform(sentences)\n",
    "    # Fit model\n",
    "    model.fit(sentences)\n",
    "\n",
    "    #Make predictions\n",
    "    yhat = model.predict(sentences)\n",
    "\n",
    "    # retrieve unique clusters\n",
    "    clusters = unique(yhat)\n",
    "    # create scatter plot for samples from each cluster\n",
    "    for cluster in clusters:\n",
    "        # get row indexes for samples with this cluster\n",
    "        row_ix = where(yhat == cluster)\n",
    "        # create scatter of these samples\n",
    "        plt.scatter(sentences[row_ix, 0], sentences[row_ix, 1])\n",
    "        # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_comments(new_pol_embs, 4, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n",
    "\n",
    "print('There are {} samples in the training set and {} samples in the test set'.format(\n",
    "X_train.shape[0], X_test.shape[0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA from sklearn PCA\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "# Plotting the results of PCA\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')\n",
    "plt.legend(loc=0);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb833273add3e7c60eb33c0608260b79a61e072ade6f02cc8d07b0a26eef8ab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
