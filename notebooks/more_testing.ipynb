{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for using the notebook\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from embedding_functions_hugo.embedding_functions import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, SpectralClustering, BisectingKMeans, AgglomerativeClustering, FeatureAgglomeration\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying datasets from different reddit pages\n",
    "\n",
    "df_gaming = pd.read_csv('../data/scrapes/gaming.csv')\n",
    "df_satis = pd.read_csv('../data/scrapes/SatisfactoryGame.csv')\n",
    "df_marauders = pd.read_csv('../data/scrapes/MaraudersGame.csv')\n",
    "df_tarkov = pd.read_csv('../data/scrapes/EscapefromTarkov.csv')\n",
    "df_politics = pd.read_csv('../data/scrapes/politics.csv')\n",
    "\n",
    "\n",
    "# Datasets post cleaning the text\n",
    "# df_politics['cleaned_text'] = prep_pipeline(df_politics, 'comment_text')\n",
    "# df_politics['short'] = shorten_sens(df_politics['cleaned_text'], 50)\n",
    "\n",
    "# Function to speed up the process: \n",
    "\n",
    "def shorten_and_clean_dataset (comment_csv, comment_column : str, desired_comment_length : int):\n",
    "    dataframe = pd.read_csv(comment_csv)\n",
    "    dataframe['cleaned_text'] = prep_pipeline(dataframe, comment_column)\n",
    "    dataframe['short'] = shorten_sens(dataframe['cleaned_text'], desired_comment_length)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sen_leng = []\n",
    "# for i in df_politics['short']:\n",
    "#     sen_leng.append(len(i.split()))\n",
    "\n",
    "\n",
    "# print(np.percentile(sen_leng, 25))\n",
    "# print(np.percentile(sen_leng, 50))\n",
    "# print(np.percentile(sen_leng, 75))\n",
    "# print(np.percentile(sen_leng, 99))\n",
    "# print(np.mean(sen_leng))\n",
    "# print(np.median(sen_leng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors = df_politics.values[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_as_npy(destination_path : str, comment_csv, comment_column : str, desired_comment_length : int):\n",
    "    '''\n",
    "    Nlp pipeline function which takes a pandas dataframe and relevant columns, performs preprocessing steps, uses sentence_transformer embeddings and saves the embeddings as a csv file.\n",
    "    '''\n",
    "    sentences = shorten_and_clean_dataset(comment_csv, comment_column, desired_comment_length)\n",
    "    embeddings = embed_comments(sentences['short'])\n",
    "    return np.save(destination_path, embeddings)\n",
    "   # return savetxt(destination_path, embeddings, delimiter = ',')\n",
    "\n",
    "### UNCOMMENT BELOW TO DO EMBEDDINGS AND SAVE THEM\n",
    "\n",
    "# save_embeddings_as_npy('../data/embeddings/politics_embeddings.npy', '../data/scrapes/politics.csv', 'comment_text', 50)\n",
    "# save_embeddings_as_npy('../data/embeddings/gaming_embeddings.npy', '../data/scrapes/gaming.csv', 'comment_text', 50)\n",
    "# save_embeddings_as_npy('../data/embeddings/marauders_embeddings.npy', '../data/scrapes/MaraudersGame.csv', 'comment_text', 50)\n",
    "# save_embeddings_as_npy('../data/embeddings/tarkov_embeddings.npy', '../data/scrapes/EscapefromTarkov.csv', 'comment_text', 50)\n",
    "# save_embeddings_as_npy('../data/embeddings/satisfactory_embeddings.npy', '../data/scrapes/SatisfactoryGame.csv', 'comment_text', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_users_embeddings(dataframe, embeddings, average_out_comments = False):\n",
    "    usernames = dataframe['comment_author']\n",
    "    user_dictionary = {}\n",
    "    for author, embedded_comment in zip(usernames, embeddings):\n",
    "        if author not in user_dictionary.keys():\n",
    "            user_dictionary[author] = []\n",
    "            user_dictionary[author].append(embedded_comment)\n",
    "        else:\n",
    "            user_dictionary[author].append(embedded_comment)\n",
    "    if average_out_comments:\n",
    "        for user in user_dictionary:\n",
    "            number_or_comments = len(user_dictionary[user])\n",
    "            user_dictionary[user] = sum(user_dictionary[user])/number_or_comments\n",
    "    return user_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING EMBEDDINGS FROM FILES\n",
    "\n",
    "politics_embeddings = np.load('../data/embeddings/politics_embeddings.npy')\n",
    "gaming_embeddings = np.load('../data/embeddings/gaming_embeddings.npy')\n",
    "marauders_embeddings = np.load('../data/embeddings/marauders_embeddings.npy')\n",
    "tarkov_embeddings = np.load('../data/embeddings/tarkov_embeddings.npy')\n",
    "\n",
    "# include below when the satisfactory embeddings are done\n",
    "# satisfactory_embeddings = np.load('../data/embeddings/satisfactory_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_user_embeddings = pair_users_embeddings(df_politics, politics_embeddings, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=2)\n",
    "# two_dimensional_embeddings = pca.fit_transform(list(politics_user_embeddings.values()))\n",
    "\n",
    "# kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "# classes = kmeans.fit_predict(two_dimensional_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dims_and_kmeans(user_embedding_pairs, num_of_dimensions):\n",
    "    '''\n",
    "    Current version only works w 2 colors. \n",
    "    '''\n",
    "    # Set PCA to desired number of dimensions\n",
    "    pca = PCA(n_components=num_of_dimensions)\n",
    "\n",
    "\n",
    "    pca_embeddings = pca.fit_transform(list(user_embedding_pairs.values()))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0) \n",
    "\n",
    "    classes = kmeans.fit_predict(pca_embeddings)\n",
    "\n",
    "\n",
    "    label_color_map = {0 : 'r',1 : 'g'}\n",
    "    label_color = [label_color_map[l] for l in classes]\n",
    "    plt.scatter(pca_embeddings[:,0], pca_embeddings[:,1], c=label_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_dims_and_kmeans(politics_user_embeddings, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_to_one_dimension_kmeans(user_embedding_pairs):\n",
    "    '''\n",
    "    Current version only works w 2 colors. \n",
    "    '''\n",
    "    # Set PCA to desired number of dimensions\n",
    "    pca = PCA(n_components=1)\n",
    "\n",
    "\n",
    "    pca_embeddings = pca.fit_transform(list(user_embedding_pairs.values()))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0) \n",
    "\n",
    "    classes = kmeans.fit_predict(pca_embeddings)\n",
    "\n",
    "    return (user_embedding_pairs.keys(), pca_embeddings)\n",
    "\n",
    "politics_user_embeddings = pair_users_embeddings(df_politics, politics_embeddings, True)\n",
    "Squeem = reduce_to_one_dimension_kmeans(politics_user_embeddings)\n",
    "x_axis = []\n",
    "for name,emb in zip(Squeem[0], Squeem[1]):\n",
    "    x_axis.append([str(name),emb])\n",
    "\n",
    "def sortie(beb):\n",
    "    return beb[0]\n",
    "x_axis = sorted(x_axis, key=sortie)\n",
    "len(x_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for making new dataframe with columns: \n",
    "# should be exportable to networkx - preferably edgelist\n",
    "# Author (node_id),pca_x-axis , post_id\n",
    "\n",
    "authors = []\n",
    "x_axis = []\n",
    "posts = []\n",
    "\n",
    "# df_politics = df_politics.drop('post_text', axis=1).dropna()\n",
    "df_politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics = df_politics.sort_values(by='comment_author')\n",
    "for ball in df_politics.iloc:\n",
    "    ID, Author = ball[2], ball[6]\n",
    "    authors.append(Author)\n",
    "    posts.append(ID)\n",
    "print(authors, posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment inspection with regards to distance to eachother\n",
    "\n",
    "#### quick prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PCA to desired number of dimensions\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "\n",
    "pca_embeddings = pca.fit_transform(list(politics_user_embeddings.values()))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "\n",
    "classes = kmeans.fit_predict(pca_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### horizontally distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_embeddings.shape)\n",
    "#print(len(politics_user_embeddings.keys()), len(politics_user_embeddings.values()))\n",
    "\n",
    "# finding indexes of rows with least and max x values\n",
    "\n",
    "x_vals = []\n",
    "for idx, row in enumerate(pca_embeddings):\n",
    "    x_val = row[0]\n",
    "    x_vals.append(x_val)\n",
    "\n",
    "# least x\n",
    "least_x = min(x_vals)\n",
    "least_x_index = np.argmin(x_vals)\n",
    "least_x_username = list(politics_user_embeddings.keys())[least_x_index]\n",
    "least_x_comments = df_politics.loc[df_politics['comment_author'] == least_x_username]\n",
    "\n",
    "max_x = max(x_vals)\n",
    "max_x_index = np.argmax(x_vals)\n",
    "max_x_username = list(politics_user_embeddings.keys())[max_x_index]\n",
    "max_x_comments = df_politics.loc[df_politics['comment_author'] == max_x_username]\n",
    "\n",
    "#print(least_x, least_x_index, least_x_username)\n",
    "#print(max_x, max_x_index, max_x_username)\n",
    "\n",
    "#print(df_politics.shape)\n",
    "#print(len(politics_user_embeddings.keys()))\n",
    "print(least_x_comments['comment_text'].values[0])\n",
    "print()\n",
    "print(max_x_comments['comment_text'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vertically distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_embeddings.shape)\n",
    "#print(len(politics_user_embeddings.keys()), len(politics_user_embeddings.values()))\n",
    "\n",
    "# finding indexes of rows with least and max y values\n",
    "\n",
    "y_vals = []\n",
    "for idx, row in enumerate(pca_embeddings):\n",
    "    y_val = row[1]\n",
    "    y_vals.append(y_val)\n",
    "\n",
    "# least y\n",
    "least_y = min(y_vals)\n",
    "least_y_index = np.argmin(y_vals)\n",
    "least_y_username = list(politics_user_embeddings.keys())[least_y_index]\n",
    "least_y_comments = df_politics.loc[df_politics['comment_author'] == least_y_username]\n",
    "\n",
    "max_y = max(y_vals)\n",
    "max_y_index = np.argmax(y_vals)\n",
    "max_y_username = list(politics_user_embeddings.keys())[max_y_index]\n",
    "max_y_comments = df_politics.loc[df_politics['comment_author'] == max_y_username]\n",
    "\n",
    "#print(least_y, least_y_index, least_y_username)\n",
    "#print(max_y, max_y_index, max_y_username)\n",
    "\n",
    "#print(df_politics.shape)\n",
    "#print(len(politics_user_embeddings.keys()))\n",
    "print(least_y_comments['comment_text'].values[0])\n",
    "print()\n",
    "print(max_y_comments['comment_text'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### most distant (in progress, will likely be expensive af to calc if checking all distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### similar comments (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(df, embeddings):\n",
    "    '''inputs:\n",
    "        - df: df to work with\n",
    "        - embeddings: embeddings to work with\n",
    "       \n",
    "       function finds all users that fit in the limits and are therefore similar,\n",
    "       then prints their comments'''\n",
    "    \n",
    "    # pairing embeddings\n",
    "    user_embeddings = pair_users_embeddings(df, embeddings, True)\n",
    "    \n",
    "    # doing pca things\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(list(user_embeddings.values()))\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "    classes = kmeans.fit_predict(pca_embeddings)\n",
    "    \n",
    "    # print blob\n",
    "    label_color_map = {0 : 'r',1 : 'g'}\n",
    "    label_color = [label_color_map[l] for l in classes]\n",
    "    plt.scatter(pca_embeddings[:,0], pca_embeddings[:,1], c=label_color)\n",
    "    \n",
    "    # finding similar things\n",
    "    # NEED TO CODE FOR FINDING ALL FAR THINGS\n",
    "    to_check = ['MIDDLE', 'LEFT', 'RIGHT', 'TOP', 'BOTTOM']\n",
    "    \n",
    "    for i in to_check:\n",
    "        \n",
    "        # finding x and y limits based off of blob\n",
    "        \n",
    "        if i == 'MIDDLE':\n",
    "            print('========== MIDDLE ==========')\n",
    "            x_lims = [-0.1, 0.1]\n",
    "            y_lims = [-0.1, 0.1]\n",
    "            \n",
    "            similar_indexes = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_val = row[0]\n",
    "                y_val = row[1]\n",
    "                \n",
    "                if x_val > x_lims[0] and x_val < x_lims[1] and y_val > y_lims[0] and y_val < y_lims[1]:\n",
    "                    similar_indexes.append(idx)\n",
    "        \n",
    "        # checks from far left and finds first 5 comments\n",
    "        elif i == 'LEFT':\n",
    "            print('========== LEFT ==========')\n",
    "            # get list of x coords for sorting\n",
    "            x_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_coords.append(row[0])\n",
    "            \n",
    "            # sorts x coords by ascending, but gives the indexes not the values\n",
    "            sorted_indexes = np.argsort(x_coords)\n",
    "            \n",
    "            similar_indexes = sorted_indexes[:5]\n",
    "        \n",
    "        # checks from far left and finds first 5 comments\n",
    "        elif i == 'RIGHT':\n",
    "            print('========== RIGHT ==========')\n",
    "            # get list of x coords for sorting\n",
    "            x_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_coords.append(row[0])\n",
    "            \n",
    "            # sorts x coords by descending, but gives the indexes not the values\n",
    "            initial_sort = np.argsort(x_coords)\n",
    "            \n",
    "            similar_indexes = initial_sort[::-1][:5] # 5 for first 5 comments\n",
    "        \n",
    "        # checks from far top and finds first 5 comments\n",
    "        elif i == 'TOP':\n",
    "            print('========== TOP ==========')\n",
    "            # get list of y coords for sorting\n",
    "            y_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                y_coords.append(row[1])\n",
    "            \n",
    "            # sorts y coords by descending, but gives the indexes not the values\n",
    "            initial_sort = np.argsort(y_coords)\n",
    "            \n",
    "            similar_indexes = initial_sort[::-1][:5] # 5 for first 5 comments\n",
    "        \n",
    "        elif i == 'BOTTOM':\n",
    "            print('========== BOTTOM ==========')\n",
    "            # get list of y coords for sorting\n",
    "            y_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                y_coords.append(row[1])\n",
    "            \n",
    "            # sorts y coords by ascending, but gives the indexes not the values\n",
    "            sorted_indexes = np.argsort(y_coords)\n",
    "            \n",
    "            similar_indexes = sorted_indexes[:5]\n",
    "            \n",
    "        # using list of similar indexes, matches with users and prints their comments\n",
    "        usernames = list()\n",
    "        for index in similar_indexes:\n",
    "            username = list(user_embeddings.keys())[index]\n",
    "            usernames.append(username)\n",
    "\n",
    "        # cleaning comments to get relevant ones in embedding space\n",
    "        df['cleaned_text'] = prep_pipeline(df, 'comment_text')\n",
    "        df['short'] = shorten_sens(df['cleaned_text'], 50)\n",
    "        \n",
    "        for username in usernames:\n",
    "            comments = df.loc[df['comment_author'] == username]\n",
    "            #print(comments['comment_text'].values[0], '\\n')\n",
    "            print(comments['short'].values[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_politics, politics_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_gaming, gaming_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_marauders, marauders_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
