{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Tutorial for reddit scraping: https://www.geeksforgeeks.org/scraping-reddit-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# ids for scraping (from christians setup)\n",
    "client_id = 'Ut5UgaAMOEWBELtYRWnw0g'\n",
    "client_secret = '5xGs1w6mav5Ke685afpG28Q8nfusmg'\n",
    "user_agent = 'polarity search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "First we initialize a read-only instance. A read-only instance can only scrape publicly available information and cannot upvote or otherwise interact like users can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-only instance\n",
    "reddit_read_only = praw.Reddit(client_id=client_id,         # your client id\n",
    "                               client_secret=client_secret,      # your client secret\n",
    "                               user_agent=user_agent)        # your user agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on a specific post\n",
    "\n",
    "This code scrapes over the comments of a specified post. It looks only at the lead comments (none of the replies to comments). It only goes over the first 112 comments for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post(url, all_comments=False):\n",
    "    # Creating a submission object\n",
    "    submission = reddit_read_only.submission(url=url)\n",
    "    \n",
    "    # should get all top level comments on the post\n",
    "    if all_comments==True:\n",
    "        submission.comments.replace_more(limit=None)\n",
    "\n",
    "    post_authors = []\n",
    "    post_comments = []\n",
    "\n",
    "    for comment in submission.comments:\n",
    "        if type(comment) == MoreComments:\n",
    "            continue\n",
    "\n",
    "        post_authors.append(comment.author)\n",
    "        post_comments.append(comment.body)\n",
    "\n",
    "    post_dict = {'author': post_authors, 'comment': post_comments}\n",
    "    post_df = pd.DataFrame(post_dict)\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iv1i25s\n",
      "iv18ujc\n",
      "iv3ks39\n",
      "iv19egq\n",
      "iv0plt0\n",
      "iv1t4z6\n",
      "iv1v1xc\n",
      "iv1aj7l\n",
      "iv1p2k7\n",
      "iv21u69\n",
      "iv3l2xe\n",
      "iv3rxbj\n"
     ]
    }
   ],
   "source": [
    "df = scrape_post(\"https://www.reddit.com/r/MaraudersGame/comments/ylxsq4/marauders_be_like/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lozsta</td>\n",
       "      <td>Why is there not a toggle to turn that off. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpossumHades</td>\n",
       "      <td>...that destroyed ÖRTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JEClockwork</td>\n",
       "      <td>For 70 years we have long lived in the shadows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l3lNova</td>\n",
       "      <td>Ok but real talk that movie was wack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sw4mpy_1</td>\n",
       "      <td>Well no more!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                            comment\n",
       "0        Lozsta  Why is there not a toggle to turn that off. I ...\n",
       "1  OpossumHades                             ...that destroyed ÖRTH\n",
       "2   JEClockwork  For 70 years we have long lived in the shadows...\n",
       "3       l3lNova               Ok but real talk that movie was wack\n",
       "4      sw4mpy_1                                   Well no more!!!!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j3vqpob\n",
      "j3vt4y9\n",
      "j3vt4qp\n",
      "j3vrddo\n",
      "j3vtqy7\n",
      "j3vs7oi\n",
      "j3w7n18\n",
      "j3vquhk\n",
      "j3vsyea\n",
      "j3w35hr\n",
      "j3vredy\n",
      "j3vzw4y\n",
      "j3vtqnp\n",
      "j3w17ku\n",
      "j3vzrk3\n",
      "j3w2729\n",
      "j3vulzh\n",
      "j3vuwow\n",
      "j3w0qro\n",
      "j3vtp8w\n",
      "j3vzazw\n",
      "j3w636b\n",
      "j3w0ghe\n",
      "j3w0uqk\n",
      "j3vzyvh\n",
      "j3w0j0s\n",
      "j3vw8qa\n",
      "j3w7n36\n",
      "j3vynpx\n",
      "j3vyelf\n",
      "j3wf8xh\n",
      "j3vz4xx\n",
      "j3vu1z2\n",
      "j3vuusy\n",
      "j3vy2li\n",
      "j3w1v3g\n",
      "j3w2d09\n",
      "j3w3hux\n",
      "j3wg94u\n",
      "j3xaxqh\n",
      "j3vr2av\n",
      "j3vxkcr\n",
      "j3vy5pa\n",
      "j3vyi7s\n",
      "j3w039k\n",
      "j3w5edi\n",
      "j3weeiz\n",
      "j3wf0ki\n",
      "j3wfsno\n",
      "j3wgiag\n",
      "j3wgnx0\n",
      "j3wm2bg\n",
      "j3wm6al\n",
      "j3yppcz\n",
      "j3yqr4c\n",
      "j3yrvib\n",
      "j3ysut3\n",
      "j3wf92w\n",
      "j3vv60h\n",
      "j3vyju0\n",
      "j3vuavn\n",
      "j3vzl4u\n",
      "j3wfdii\n",
      "j3vwr6f\n",
      "j3vxliz\n",
      "j3vyx60\n",
      "j3w0prt\n",
      "j3w0rbi\n",
      "j3w0wd2\n",
      "j3w1867\n",
      "j3w1cj9\n",
      "j3w2o1j\n",
      "j3w2yl1\n",
      "j3w358c\n",
      "j3w3dzg\n",
      "j3w4x42\n",
      "j3wankp\n",
      "j3wdltc\n",
      "j3wfipg\n",
      "j3wlmrp\n",
      "j3wnddi\n",
      "j3wnez6\n",
      "j3wng8p\n",
      "j3wnil8\n",
      "j3wog0e\n",
      "j3wp3ff\n",
      "j3wvi9k\n",
      "j3wy6sx\n",
      "j3wyb6d\n",
      "j3wyc3u\n",
      "j3x33b4\n",
      "j3x39dh\n",
      "j3x5fea\n",
      "j3x5fyy\n",
      "j3x5kk7\n",
      "j3xai9c\n",
      "j3xf8jz\n",
      "j3xk240\n",
      "j3xlnlw\n",
      "j3xlta8\n",
      "j3xm3x8\n",
      "j3xmutr\n",
      "j3xnppk\n",
      "j3xqakf\n",
      "j3xsdvi\n",
      "j3y1jim\n",
      "j3yguck\n",
      "j3yhiu7\n",
      "j3ypobl\n",
      "<MoreComments count=2421, children=['j3wlfya', 'j3wbsar', 'j3w8jd0', '...']>\n"
     ]
    }
   ],
   "source": [
    "df = scrape_post(\"https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lozsta</td>\n",
       "      <td>Why is there not a toggle to turn that off. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpossumHades</td>\n",
       "      <td>...that destroyed ÖRTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JEClockwork</td>\n",
       "      <td>For 70 years we have long lived in the shadows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l3lNova</td>\n",
       "      <td>Ok but real talk that movie was wack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sw4mpy_1</td>\n",
       "      <td>Well no more!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                            comment\n",
       "0        Lozsta  Why is there not a toggle to turn that off. I ...\n",
       "1  OpossumHades                             ...that destroyed ÖRTH\n",
       "2   JEClockwork  For 70 years we have long lived in the shadows...\n",
       "3       l3lNova               Ok but real talk that movie was wack\n",
       "4      sw4mpy_1                                   Well no more!!!!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting top month posts on specified subreddit\n",
    "This code grabs the top 100 posts of the past month and saves various information on them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_month(subreddit, ppsr=100):\n",
    "    # specifying subreddit\n",
    "    subreddit = reddit_read_only.subreddit(subreddit)\n",
    "\n",
    "    # Specifying to look at top posts of the current month\n",
    "    posts = subreddit.top(\"month\", limit=ppsr)\n",
    "\n",
    "    # Initializing dictionary to save post data to\n",
    "    posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "                  \"ID\": [], \"Score\": [],\n",
    "                  \"Total Comments\": [], \"Post URL\": []\n",
    "                  }\n",
    "\n",
    "    # Loop for saving post details\n",
    "    for post in posts:\n",
    "        # Title of each post\n",
    "        posts_dict[\"Title\"].append(post.title)\n",
    "\n",
    "        # Text inside a post\n",
    "        posts_dict[\"Post Text\"].append(post.selftext)\n",
    "\n",
    "        # Unique ID of each post\n",
    "        posts_dict[\"ID\"].append(post.id)\n",
    "\n",
    "        # The score of a post\n",
    "        posts_dict[\"Score\"].append(post.score)\n",
    "\n",
    "        # Total number of comments inside the post\n",
    "        posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "\n",
    "        # URL of each post\n",
    "        posts_dict[\"Post URL\"].append('https://www.reddit.com'+f'{post.permalink}')\n",
    "    \n",
    "    return posts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_3972\\2075428899.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n"
     ]
    }
   ],
   "source": [
    "dict_ = scrape_top_month('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Must Pay Hillary Clinton $171,631 in Legal Fees Over Bogus Lawsuit\n",
      "\n",
      "10h1dc6\n",
      "68515\n",
      "2003\n",
      "https://www.reddit.com/r/politics/comments/10h1dc6/trump_must_pay_hillary_clinton_171631_in_legal/\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_3972\\2075428899.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n"
     ]
    }
   ],
   "source": [
    "dict_ = scrape_top_month('politics', ppsr=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Must Pay Hillary Clinton $171,631 in Legal Fees Over Bogus Lawsuit\n",
      "\n",
      "10h1dc6\n",
      "68509\n",
      "2003\n",
      "https://www.reddit.com/r/politics/comments/10h1dc6/trump_must_pay_hillary_clinton_171631_in_legal/\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on top monthly posts on multiple subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_save(subreddits, ppsr=100, all_comments=False):\n",
    "    '''scrapes and saves subreddit comments to csv files\n",
    "       Naming convention is: SUBREDDIT_POSTID.csv / SUBREDDIT_POSTID_INFO.txt'''\n",
    "    \n",
    "    \n",
    "    if all_comments==False:\n",
    "        print(f'Scraping {ppsr} posts per subreddit and ~100 comments per post')\n",
    "    else:\n",
    "        print(f'Scraping {ppsr} posts per subreddit and all comments per post')\n",
    "    \n",
    "    # looping through subreddits\n",
    "    for subreddit in subreddits:\n",
    "        print(f'Scraping r/{subreddit}...')\n",
    "        \n",
    "        # initialize dictionary for saving all comments and post info\n",
    "        sub_dict = {'post_title': [],\n",
    "                    'post_text': [],\n",
    "                    'post_id': [],\n",
    "                    'post_score': [],\n",
    "                    'post_total_comments': [],\n",
    "                    'post_url': [],\n",
    "                    'comment_author': [],\n",
    "                    'comment_text': []}\n",
    "        \n",
    "        posts_dict = scrape_top_month(subreddit, ppsr) # getting top of the month post info\n",
    "        \n",
    "        # looping through posts\n",
    "        for idx, url in enumerate(posts_dict['Post URL']):\n",
    "            \n",
    "            # df for comments on the post\n",
    "            comment_df = scrape_post(url, all_comments=all_comments)\n",
    "            \n",
    "            # looping through comments on post and appending all comment info to sub_dict\n",
    "            for row_idx, row in comment_df.iterrows():\n",
    "                sub_dict['post_title'].append(posts_dict['Title'][idx])\n",
    "                sub_dict['post_text'].append(posts_dict['Post Text'][idx])\n",
    "                sub_dict['post_id'].append(posts_dict['ID'][idx])\n",
    "                sub_dict['post_score'].append(posts_dict['Score'][idx])\n",
    "                sub_dict['post_total_comments'].append(posts_dict['Total Comments'][idx])\n",
    "                sub_dict['post_url'].append(posts_dict['Post URL'][idx])\n",
    "                sub_dict['comment_author'].append(row['author'])\n",
    "                sub_dict['comment_text'].append(row['comment'])\n",
    "            \n",
    "        # changing sub_dict to pandas dataframe\n",
    "        sub_df = pd.DataFrame.from_dict(sub_dict)\n",
    "        \n",
    "        # saving to csv\n",
    "        sub_df.to_csv(f'../data/{subreddit}.csv', index=False)\n",
    "        \n",
    "    print('Done!')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 100 posts per subreddit and ~100 comments per post\n",
      "Scraping r/politics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_3972\\2075428899.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/gaming...\n",
      "Scraping r/MaraudersGame...\n",
      "Scraping r/EscapefromTarkov...\n",
      "Scraping r/SatisfactoryGame...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scrape_multiple_save(['politics', 'gaming', 'MaraudersGame', 'EscapefromTarkov', 'SatisfactoryGame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae25dc04c095084e91b1f2b0dfe81b87ca9c6d3934fc1ad15c6958801e08a15b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
