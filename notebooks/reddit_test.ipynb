{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Tutorial for reddit scraping: https://www.geeksforgeeks.org/scraping-reddit-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# ids for scraping (from christians setup)\n",
    "client_id = 'Ut5UgaAMOEWBELtYRWnw0g'\n",
    "client_secret = '5xGs1w6mav5Ke685afpG28Q8nfusmg'\n",
    "user_agent = 'polarity search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "First we initialize a read-only instance. A read-only instance can only scrape publicly available information and cannot upvote or otherwise interact like users can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-only instance\n",
    "reddit_read_only = praw.Reddit(client_id=client_id,         # your client id\n",
    "                               client_secret=client_secret,      # your client secret\n",
    "                               user_agent=user_agent)        # your user agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on a specific post\n",
    "\n",
    "This code scrapes over the comments of a specified post. It looks only at the lead comments (none of the replies to comments). It only goes over the first 112 comments for some reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_post(url):\n",
    "    # Creating a submission object\n",
    "    submission = reddit_read_only.submission(url=url)\n",
    "\n",
    "    post_authors = []\n",
    "    post_comments = []\n",
    "\n",
    "    for comment in submission.comments:\n",
    "        if type(comment) == MoreComments:\n",
    "            continue\n",
    "\n",
    "        post_authors.append(comment.author)\n",
    "        post_comments.append(comment.body)\n",
    "\n",
    "    post_dict = {'author': post_authors, 'comment': post_comments}\n",
    "    post_df = pd.DataFrame(post_dict)\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>\\nAs a reminder, this subreddit [is for civil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PandaMuffin1</td>\n",
       "      <td>&gt;  Hours after Middlebrooks' filing became pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dagonet_the_Motley</td>\n",
       "      <td>LOL imagine all the suckers who gave money to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lufecaep</td>\n",
       "      <td>They should double it every time he tries to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FortySixAndYou</td>\n",
       "      <td>So, this is on top of the $1M in sanctions tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                            comment\n",
       "0       AutoModerator  \\nAs a reminder, this subreddit [is for civil ...\n",
       "1        PandaMuffin1  >  Hours after Middlebrooks' filing became pub...\n",
       "2  Dagonet_the_Motley  LOL imagine all the suckers who gave money to ...\n",
       "3            lufecaep  They should double it every time he tries to a...\n",
       "4      FortySixAndYou  So, this is on top of the $1M in sanctions tha..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = scrape_post(\"https://www.reddit.com/r/politics/comments/10h1dc6/trump_must_pay_hillary_clinton_171631_in_legal/\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting top month posts on specified subreddit\n",
    "This code grabs the top 100 posts of the past month and saves various information on them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_month(subreddit):\n",
    "    # specifying subreddit\n",
    "    subreddit = reddit_read_only.subreddit(subreddit)\n",
    "\n",
    "    # Specifying to look at top posts of the current month\n",
    "    posts = subreddit.top(\"month\")\n",
    "\n",
    "    # Initializing dictionary to save post data to\n",
    "    posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "                  \"ID\": [], \"Score\": [],\n",
    "                  \"Total Comments\": [], \"Post URL\": []\n",
    "                  }\n",
    "\n",
    "    # Loop for saving post details\n",
    "    for post in posts:\n",
    "        # Title of each post\n",
    "        posts_dict[\"Title\"].append(post.title)\n",
    "\n",
    "        # Text inside a post\n",
    "        posts_dict[\"Post Text\"].append(post.selftext)\n",
    "\n",
    "        # Unique ID of each post\n",
    "        posts_dict[\"ID\"].append(post.id)\n",
    "\n",
    "        # The score of a post\n",
    "        posts_dict[\"Score\"].append(post.score)\n",
    "\n",
    "        # Total number of comments inside the post\n",
    "        posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "\n",
    "        # URL of each post\n",
    "        posts_dict[\"Post URL\"].append('https://www.reddit.com'+f'{post.permalink}')\n",
    "    \n",
    "    return posts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_18532\\397523184.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\")\n"
     ]
    }
   ],
   "source": [
    "dict_ = scrape_top_month('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The American public no longer believes the Supreme Court is impartial\n",
      "\n",
      "1092xhl\n",
      "81842\n",
      "3812\n",
      "https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on top monthly posts on multiple subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "def scrape_multiple(subreddits):\n",
    "    \n",
    "    subreddit_df_dict = dict()\n",
    "    \n",
    "    # looping through subreddits\n",
    "    for subreddit in subreddits:\n",
    "        posts_dict = scrape_top_month(subreddit) # getting top of the month post info\n",
    "        \n",
    "        subreddit_post_dfs = []\n",
    "        \n",
    "        # looping through posts\n",
    "        for idx, url in enumerate(posts_dict['Post URL']):\n",
    "            # information about the post\n",
    "            post_info = [posts_dict['Title'][idx],\n",
    "                         posts_dict['Post Text'][idx],\n",
    "                         posts_dict['ID'][idx],\n",
    "                         posts_dict['Score'][idx],\n",
    "                         posts_dict['Total Comments'][idx],\n",
    "                         posts_dict['Post URL'][idx]]\n",
    "            \n",
    "            # df for comments on the post\n",
    "            comment_df = scrape_post(url)\n",
    "            \n",
    "            pre_df_dict = {'post_info': post_info, 'comment_df': comment_df}\n",
    "            post_df = pd.DataFrame(pre_df_dict)\n",
    "        \n",
    "            subreddit_post_dfs.append(post_df)\n",
    "        \n",
    "        subreddit_df_dict[subreddit] = subreddit_post_dfs\n",
    "    \n",
    "    subbreddit_df = pd.DataFrame(subredit_df_dict)\n",
    "    \n",
    "    return subreddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_save(subreddits):\n",
    "    '''scrapes and saves subreddit comments to csv files\n",
    "       Naming convention is: SUBREDDIT_POSTID.csv / SUBREDDIT_POSTID_INFO.txt'''\n",
    "    # looping through subreddits\n",
    "    for subreddit in subreddits:\n",
    "        posts_dict = scrape_top_month(subreddit) # getting top of the month post info\n",
    "        \n",
    "        # looping through posts\n",
    "        for idx, url in enumerate(posts_dict['Post URL']):\n",
    "            # information about the post\n",
    "            post_info = [posts_dict['Title'][idx],\n",
    "                         posts_dict['Post Text'][idx],\n",
    "                         posts_dict['ID'][idx],\n",
    "                         posts_dict['Score'][idx],\n",
    "                         posts_dict['Total Comments'][idx],\n",
    "                         posts_dict['Post URL'][idx]]\n",
    "            \n",
    "            # df for comments on the post\n",
    "            comment_df = scrape_post(url)\n",
    "            \n",
    "            comment_df.to_csv(f'../data/{subreddit}_{post_info[2]}.csv', index=False)\n",
    "            \n",
    "    print('Done!')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_18532\\397523184.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scrape_multiple_save(['politics', 'gaming', 'MaraudersGame', 'EscapefromTarkov', 'SatisfactoryGame'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aidan\\AppData\\Local\\Temp\\ipykernel_18532\\397523184.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 1, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolitics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgaming\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 25\u001b[0m, in \u001b[0;36mscrape_multiple\u001b[1;34m(subreddits)\u001b[0m\n\u001b[0;32m     22\u001b[0m     comment_df \u001b[38;5;241m=\u001b[39m scrape_post(url)\n\u001b[0;32m     24\u001b[0m     pre_df_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost_info\u001b[39m\u001b[38;5;124m'\u001b[39m: post_info, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment_df\u001b[39m\u001b[38;5;124m'\u001b[39m: comment_df}\n\u001b[1;32m---> 25\u001b[0m     post_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpre_df_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     subreddit_post_dfs\u001b[38;5;241m.\u001b[39mappend(post_df)\n\u001b[0;32m     29\u001b[0m subreddit_df_dict[subreddit] \u001b[38;5;241m=\u001b[39m subreddit_post_dfs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\internals\\construction.py:125\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    122\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\internals\\construction.py:625\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    622\u001b[0m             val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[0;32m    623\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m--> 625\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_cast_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    630\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\construction.py:593\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[0;32m    591\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m _try_cast(data, dtype, copy, raise_cast_failure)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 593\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subarr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    595\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:130\u001b[0m, in \u001b[0;36mmaybe_convert_platform\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m _dtype_obj:\n\u001b[0;32m    129\u001b[0m     arr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, arr)\n\u001b[1;32m--> 130\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\reddit_scraping\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2395\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
     ]
    }
   ],
   "source": [
    "df = scrape_multiple(['politics', 'gaming'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae25dc04c095084e91b1f2b0dfe81b87ca9c6d3934fc1ad15c6958801e08a15b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
