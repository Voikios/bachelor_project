{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Tutorial for reddit scraping: https://www.geeksforgeeks.org/scraping-reddit-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ids for scraping (from christians setup)\n",
    "client_id = 'Ut5UgaAMOEWBELtYRWnw0g'\n",
    "client_secret = '5xGs1w6mav5Ke685afpG28Q8nfusmg'\n",
    "user_agent = 'polarity search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "First we initialize a read-only instance. A read-only instance can only scrape publicly available information and cannot upvote or otherwise interact like users can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-only instance\n",
    "reddit_read_only = praw.Reddit(client_id=client_id,         # your client id\n",
    "                               client_secret=client_secret,      # your client secret\n",
    "                               user_agent=user_agent)        # your user agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on a specific post\n",
    "\n",
    "This code scrapes over the comments of a specified post. It looks only at the top-level comments (none of the replies to comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post(url, n=100):\n",
    "    '''given a url:\n",
    "     - scrapes n number of top-level comments\n",
    "     - comment author (username) and comment body ()\n",
    "     - outputs a pandas dataframe'''\n",
    "\n",
    "    # Creating a submission object\n",
    "    submission = reddit_read_only.submission(url=url)\n",
    "    \n",
    "    # should get all top level comments on the post\n",
    "    #if all_comments==True:\n",
    "    #    submission.comments.replace_more(limit=None)\n",
    "\n",
    "    post_authors = []\n",
    "    post_comments = []\n",
    "\n",
    "    # specifying how many times we should \"load more comments\"\n",
    "    limit = round(n/20) # n/20 ensures we hit load more enough times to scrape the number of comments we want \n",
    "    submission.comments.replace_more(limit=limit, threshold=0)\n",
    "\n",
    "    count = 0 # tracking how many comments have been scraped\n",
    "    for comment in submission.comments: # iterates only over top level comments\n",
    "        # stops counting when n amount of comments have been scraped\n",
    "        if count == n:\n",
    "            break\n",
    "\n",
    "        if type(comment) == MoreComments:\n",
    "            continue\n",
    "\n",
    "        post_authors.append(comment.author)\n",
    "        post_comments.append(comment.body)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    post_dict = {'author': post_authors, 'comment': post_comments}\n",
    "    post_df = pd.DataFrame(post_dict)\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of how scraping a post works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping a post without much content - takes <1 second\n",
    "# there are 12 top level comments\n",
    "df = scrape_post(\"https://www.reddit.com/r/MaraudersGame/comments/ylxsq4/marauders_be_like/\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping a decently sized post (function default scrapes 100 comments) - takes ~10 seconds\n",
    "df = scrape_post(\"https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping 200 comments from a decently sized post (post has 3.8k comments) - takes ~15 seconds\n",
    "df = scrape_post(\"https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\", n=200)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidan: idk what below line was so i just commented it out\n",
    "# praw.models.reddit.submission.Submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting top month posts on specified subreddit\n",
    "This code grabs the top 100 posts of the past month and saves various information on them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_month(subreddit, ppsr=100):\n",
    "    # specifying subreddit\n",
    "    subreddit = reddit_read_only.subreddit(subreddit)\n",
    "\n",
    "    # Specifying to look at top posts of the current month\n",
    "    posts = subreddit.top(\"month\", limit=ppsr)\n",
    "\n",
    "    # Initializing dictionary to save post data to\n",
    "    posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "                  \"ID\": [], \"Score\": [],\n",
    "                  \"Total Comments\": [], \"Post URL\": [], 'Post_author' : []\n",
    "                  }\n",
    "\n",
    "    # Loop for saving post details\n",
    "    for post in posts:\n",
    "        # print(post)\n",
    "        # Title of each post\n",
    "        posts_dict[\"Title\"].append(post.title)\n",
    "\n",
    "        # Text inside a post\n",
    "        posts_dict[\"Post Text\"].append(post.selftext)\n",
    "\n",
    "        # Unique ID of each post\n",
    "        posts_dict[\"ID\"].append(post.id)\n",
    "\n",
    "        # The score of a post\n",
    "        posts_dict[\"Score\"].append(post.score)\n",
    "\n",
    "        # Total number of comments inside the post\n",
    "        posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "\n",
    "        # print(post.author)\n",
    "        # Author of the post\n",
    "        posts_dict['Post_author'].append(post.author)\n",
    "\n",
    "        # URL of each post\n",
    "        # print('https://www.reddit.com'+f'{post.permalink}')\n",
    "        posts_dict[\"Post URL\"].append('https://www.reddit.com'+f'{post.permalink}')\n",
    "        \n",
    "    return posts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = scrape_top_month('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))\n",
    "print(dict_['Post_author'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = scrape_top_month('politics', ppsr=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))\n",
    "print(dict_['Post_author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on top monthly posts on multiple subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_save(subreddits, ppsr=100, all_comments=False):\n",
    "    '''scrapes and saves subreddit comments to csv files\n",
    "       Naming convention is: SUBREDDIT_POSTID.csv / SUBREDDIT_POSTID_INFO.txt'''\n",
    "    \n",
    "    \n",
    "    if all_comments==False:\n",
    "        print(f'Scraping {ppsr} posts per subreddit and ~100 comments per post')\n",
    "    else:\n",
    "        print(f'Scraping {ppsr} posts per subreddit and all comments per post')\n",
    "    \n",
    "    # looping through subreddits\n",
    "    for subreddit in subreddits:\n",
    "        print(f'Scraping r/{subreddit}...')\n",
    "        \n",
    "        # initialize dictionary for saving all comments and post info\n",
    "        sub_dict = {'post_title': [],\n",
    "                    'post_text': [],\n",
    "                    'post_id': [],\n",
    "                    'post_score': [],\n",
    "                    'post_total_comments': [],\n",
    "                    'post_url': [],\n",
    "                    'comment_author': [],\n",
    "                    'comment_text': [], \n",
    "                    'post_author' : []}\n",
    "        \n",
    "        posts_dict = scrape_top_month(subreddit, ppsr) # getting top of the month post info\n",
    "        \n",
    "        # looping through posts\n",
    "        for idx, url in tqdm(enumerate(posts_dict['Post URL']),):\n",
    "            \n",
    "            # df for comments on the post\n",
    "            comment_df = scrape_post(url, all_comments=all_comments)\n",
    "            \n",
    "            # looping through comments on post and appending all comment info to sub_dict\n",
    "            for row_idx, row in comment_df.iterrows():\n",
    "                sub_dict['post_title'].append(posts_dict['Title'][idx])\n",
    "                sub_dict['post_text'].append(posts_dict['Post Text'][idx])\n",
    "                sub_dict['post_id'].append(posts_dict['ID'][idx])\n",
    "                sub_dict['post_score'].append(posts_dict['Score'][idx])\n",
    "                sub_dict['post_total_comments'].append(posts_dict['Total Comments'][idx])\n",
    "                sub_dict['post_url'].append(posts_dict['Post URL'][idx])\n",
    "                sub_dict['comment_author'].append(row['author'])\n",
    "                sub_dict['comment_text'].append(row['comment'])\n",
    "                sub_dict['post_author'].append(posts_dict['Post_author'][idx])\n",
    "            \n",
    "        # changing sub_dict to pandas dataframe\n",
    "        global sub_df\n",
    "        sub_df = pd.DataFrame.from_dict(sub_dict)\n",
    "\n",
    "        # saving to csv\n",
    "        #sub_df.to_csv(f'../data/28feb/scrapes/{subreddit}.csv', index=False)\n",
    "        \n",
    "    print('Done!')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_multiple_save(['politics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(f'../data/28feb/scrapes/politics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
